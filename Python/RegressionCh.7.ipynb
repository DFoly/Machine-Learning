{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries we need\n",
    "# we will use sklearn, numpy and matplotlib\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_boston\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print boston.DESCR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston.data[1:200,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller Datset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array(boston.data[:, [0,1,2,4,5,6,7,9,10,11]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(boston.data[:,5], boston.target, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run regression of house prices on number of rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506L, 1L)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = boston.data[:,5]\n",
    "x = np.array([[v] for v in x])\n",
    "y = boston.target\n",
    "x.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.6533504])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope,_,_,_= np.linalg.lstsq(x,y)\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add constant parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.10210898118 -34.6706207764\n"
     ]
    }
   ],
   "source": [
    "x = boston.data[:,5]\n",
    "x = np.array([[v,1] for v in x])\n",
    "y = boston.target\n",
    "(slope,bias),_,_,_ = np.linalg.lstsq(x,y)\n",
    "print slope, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506L, 1L)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape y as I think it is messing up grad descent calc\n",
    "# make sure its m*1\n",
    "y = np.reshape(y, (x.shape[0],1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.60307138922\n"
     ]
    }
   ],
   "source": [
    "(slope,bias),total_error,_,_ = np.linalg.lstsq(x,y)\n",
    "rmse = np.sqrt(total_error[0]/len(x))\n",
    "print rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def Normalise(x):\n",
    "    # N.B should not have constant yet\n",
    "    # returns matrix with each column mean = 0, std = 1\n",
    "    x_norm = (x - x.mean(axis=0))/x.std(axis=0,ddof=1)\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.4148611  -0.48724019 -0.59279438 -0.73953036  1.28144555\n",
      " -0.26554897  0.55660905 -0.98635338 -0.3027945   0.39603507]\n"
     ]
    }
   ],
   "source": [
    "x[2,0:8]\n",
    "x_norm = Normalise(x)\n",
    "x_norm\n",
    "x_norm = np.insert(x_norm,0,1,1) # must append col of ones\n",
    "print x_norm[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First do regression like book, then try normal eqns approach\n",
    "X is normalised from this point on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.31673233]\n"
     ]
    }
   ],
   "source": [
    "x = x_norm\n",
    "y = boston.target\n",
    "s,total_error,_,_ = np.linalg.lstsq(x,y)\n",
    "rmse = np.sqrt(total_error/len(x))\n",
    "print rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.41730039,  0.28454827, -1.28663623],\n",
       "       [ 1.        , -0.41485878, -0.48724019, -0.59279438],\n",
       "       [ 1.        , -0.4148611 , -0.48724019, -0.59279438]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "lr.fit(x,y)\n",
    "p = map(lr.predict,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set: 266.700512964\n"
     ]
    }
   ],
   "source": [
    "e = p-y\n",
    "total_error = np.sum(e*e)\n",
    "rmse_train = np.sqrt(total_error/len(p))\n",
    "print('RMSE on training set: {}'.format(rmse_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266.700512964\n"
     ]
    }
   ],
   "source": [
    "print np.sqrt(total_error/len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(len(x), n_folds = 10)\n",
    "err = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop that calculates regressions on different variables and selects the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse on 10 fold cross validation: 65.1374830957\n"
     ]
    }
   ],
   "source": [
    "for train,test in kf:\n",
    "    lr.fit(x[train], y[train])\n",
    "    p = map(lr.predict, x[test])\n",
    "    e = p-y[test]\n",
    "    err += np.sum(e*e)\n",
    "rmse_10cv = np.sqrt(err/len(x))\n",
    "print('rmse on 10 fold cross validation: {}'.format(rmse_10cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Regression (Ridge and Lasso and Eastic Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "en = ElasticNet(fit_intercept = True, alpha = 0.5)\n",
    "en.fit(x,y)\n",
    "p = map(en.predict,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the training set: 247.821435539\n"
     ]
    }
   ],
   "source": [
    "e = p-y\n",
    "err = np.sum(e*e)\n",
    "rmse = np.sqrt(err/len(x))\n",
    "print('RMSE for the training set: {}'.format(rmse)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Elastic net: 61.3624803056\n"
     ]
    }
   ],
   "source": [
    "kf1 = KFold(len(x), n_folds=10)\n",
    "err=0\n",
    "for train,test in kf1:\n",
    "    en.fit(x[train],y[train])\n",
    "    p = map(en.predict, x[test])\n",
    "    e = p-y[test]\n",
    "    err += np.sum(e*e)\n",
    "rmse_10cv1 = np.sqrt(err/len(x))\n",
    "print('RMSE for Elastic net: {}'.format(rmse_10cv1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve model Analytically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.53280632  -1.06817758   0.76463106  -0.55720013  -2.02234253\n",
      "   4.57127478  -1.35639418  -3.33394924   0.03534378  -2.06091784\n",
      "   1.22754427]\n"
     ]
    }
   ],
   "source": [
    "# using solve\n",
    "#x = boston.data\n",
    "#x = np.insert(x,0,1,1) # append col on ones\n",
    "xx = x.T.dot(x)\n",
    "xy = x.T.dot(y)\n",
    "Beta = np.linalg.solve(xx,xy)\n",
    "print Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22.53280632  -1.06817758   0.76463106  -0.55720013  -2.02234253\n",
      "   4.57127478  -1.35639418  -3.33394924   0.03534378  -2.06091784\n",
      "   1.22754427]\n"
     ]
    }
   ],
   "source": [
    "# using inverse and normalisation\n",
    "xx1 =  np.linalg.inv(x.T.dot(x))\n",
    "xy = x.T.dot(y)\n",
    "Beta1 = xx1.dot(xy)\n",
    "print Beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         -0.48724019 -0.59279438 -0.73953036] [ 1.         -0.48724019 -0.59279438 -0.73953036]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularise with Identity matrix size of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]\n"
     ]
    }
   ],
   "source": [
    "IdentSize = x.shape[1] # no of cols of x\n",
    "IdentMatrix = np.zeros((IdentSize,IdentSize))\n",
    "np.fill_diagonal(IdentMatrix,1)\n",
    "IdentMatrix[0,0] = 0  # don't regularize the constant\n",
    "# experiment with different lambda values\n",
    "# could also write a for loop to calculate best lambda\n",
    "lamb = 0.3  # if this is 0 then we just have normal OLS\n",
    "lamb1 = np.arange(0,1,0.1)\n",
    "print lamb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularised Beta: [ 22.53280632  -1.0670519    0.76304763  -0.55710947  -2.01604802\n",
      "   4.56955182  -1.35383986  -3.32447592   0.03234886  -2.05965736\n",
      "   1.22700085]\n"
     ]
    }
   ],
   "source": [
    "## Calculate normal eqns\n",
    "xxreg = x.T.dot(x) + lamb*IdentMatrix\n",
    "xy = x.T.dot(y)\n",
    "BetaReg = np.linalg.solve(xxreg,xy)\n",
    "print('Regularised Beta: {}'.format(BetaReg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with regularisation:[5.3167323344854402, 5.3167327157333206, 5.3167338569370557, 5.3167357543043474, 5.3167384040661823, 5.3167418024766597, 5.3167459458128254, 5.3167508303745024, 5.3167564524841291, 5.3167628084865957]\n"
     ]
    }
   ],
   "source": [
    "## Caluculate RMSE\n",
    "rmse = []\n",
    "for i in lamb1:\n",
    "    xxreg = x.T.dot(x) + i*IdentMatrix\n",
    "    xy = x.T.dot(y)\n",
    "    BetaReg = np.linalg.solve(xxreg,xy)\n",
    "    err = (y- x.dot(BetaReg))\n",
    "    RSS = np.sum(err.T * err)\n",
    "    RMSE = (np.sqrt(RSS/len(x)))\n",
    "    rmse.append(RMSE)\n",
    "    #rmse.append(rmse)\n",
    "print('RMSE with regularisation:{}'.format(rmse))\n",
    "# For some reason I get the correct answer doing it manually rather than using \n",
    "# sklearn tools\n",
    "########################################\n",
    "## Lambda = 0.4 : RMSE = 4.684\n",
    "## Lambda = 0.3 : RMSE = 4.684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next we want to use gradient descent to calculate optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11L,)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "We need to find a cost function J and then apply gradient descent iteratively\n",
    "Our cost function will be the residual sum of squares averaged and then we will apply regularisation \n",
    "to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506L, 1L)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_theta = np.zeros((x.shape[1],1))\n",
    "initial_theta\n",
    "h = x.dot(initial_theta)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to compute the cost\n",
    "# note, make sure specify y is M*1 as this was messing with calc before\n",
    "def ComputeCost(theta,x,y):\n",
    "    m = len(y)\n",
    "    J = 0\n",
    "    h = x.dot(theta)\n",
    "    e = (y-h)\n",
    "    #J = (0.5*m) * np.sum(np.square(h-y))\n",
    "    J = (0.5*m) * np.sum(e.T.dot(e))\n",
    "    return J    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506L, 1L)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regularised cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regCostFunction(theta,x,y,lamb):\n",
    "    m =  len(y)\n",
    "    J = 0\n",
    "    h = x.dot(theta)\n",
    "    e = (y-h)\n",
    "    # dont regularise constant\n",
    "    J = (0.5*m) * np.sum(e.T.dot(e)) + (lamb*0.5*m) * np.sum(np.square(theta[1:]))\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x.dot(initial_theta)\n",
    "temp_theta = np.zeros(initial_theta.shape)\n",
    "temp_theta\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75805464.020000026"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ComputeCost(initial_theta,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75805464.020000026"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regCostFunction(initial_theta,x,y,0.2) # not much affect since our model is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we need Theta,x,y,J. We calculate the gradient at each theta value and then\n",
    "Compute the cost. We iterate until we get convergence or for the number of specified iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kind of strange answer\n",
    "def gradDescent(theta,x,y,alpha,runs):\n",
    "    # intialise parameters\n",
    "    m = len(y)\n",
    "    J_hist = np.zeros(runs)\n",
    "    \n",
    "    #main loop\n",
    "    for i in np.arange(runs):\n",
    "        h = x.dot(theta)\n",
    "        temp_theta = np.zeros(theta.shape)\n",
    "        # derivative of cost function: gradient (vector of partial derivatives)\n",
    "        # second loop to calculate gradient at each theta\n",
    "        for j in np.arange(theta.shape[0]):\n",
    "            temp_theta[j] = temp_theta[j] - ((1/float(m)) * alpha * np.sum((h-y) * x[:,j]))\n",
    "    \n",
    "        theta = temp_theta\n",
    "        J_hist[i] = ComputeCost(theta,x,y)\n",
    "    return J_hist,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('theta: ', array([  1.59916880e+11]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGShJREFUeJzt3X+w3XV95/Hni4RgQEiGgSIJUKLeDIRC8BewWvX6o0zq\nuED3D0BXmirrTget1J21JjhT4j8OO7ZbqR3s2ALittChVBF3EMlS7up2lWgliAmRhNkgQRJYA0Rx\nhQTe+8f5XnMSbn6cm3Pv9957no+ZDN/v93zP97zPGc0rnx/fzzdVhSRJvTis7QIkSdOP4SFJ6pnh\nIUnqmeEhSeqZ4SFJ6pnhIUnq2YSHR5IbkmxL8mDXsc8meSjJA0m+kmRe12srk2xMsiHJ+V3H35Dk\nwea1aye6bknSvk1Gy+NGYNlex+4GzqiqpcDDwEqAJEuAS4AlzXuuS5LmPV8ALq+qIWAoyd7XlCRN\nkgkPj6r6NvD0XsdWV9VLze59wEnN9oXALVW1s6o2A5uAc5OcCBxdVWua874MXDTRtUuSxjYVxjw+\nBNzZbC8AtnS9tgVYOMbxx5vjkqQWtBoeST4FvFBVN7dZhySpN7Pb+uAkfwC8B3hX1+HHgZO79k+i\n0+J4nN1dW6PHH9/HdV2sS5LGoapy4LM6Wml5NIPdnwAurKpfdb10B3BpkjlJFgFDwJqq2grsSHJu\nM4B+GXD7vq5fVf6p4uqrr269hqnyx9/C38LfYv9/ejXhLY8ktwBvB45L8hhwNZ3ZVXOA1c1kqu9U\n1RVVtT7JrcB6YBdwRe3+VlcAXwLmAndW1V0TXbskaWwTHh5V9b4xDt+wn/M/A3xmjOP/CpzZx9Ik\nSeM0FWZbaYIMDw+3XcKU4W+xm7/Fbv4W45fx9HVNZUlqpn0nSZpoSaipPmAuSZreDA9JUs8MD0lS\nzwwPSVLPDA9JUs8MD0lSz2ZkeLz4YtsVSNLMNiPDY9OmtiuQpJltRobHAw+0XYEkzWyGhySpZ4aH\nJKlnMzI81q5tuwJJmtlmZHjs2AE/+1nbVUjSzDUjw2PpUruuJGkiGR6SpJ4ZHpKknhkekqSezcgn\nCT73XHHccfDMMzBnTtsVSdLU55MEgSOPhFNOgQ0b2q5EkmamGRkeAGefbdeVJE2UGRsejntI0sSZ\n8PBIckOSbUke7Dp2bJLVSR5OcneS+V2vrUyyMcmGJOd3HX9Dkgeb16490OcaHpI0cSaj5XEjsGyv\nYyuA1VW1GLin2SfJEuASYEnznuuSjA7gfAG4vKqGgKEke19zD6PhMcPmA0jSlDDh4VFV3wae3uvw\nBcBNzfZNwEXN9oXALVW1s6o2A5uAc5OcCBxdVWua877c9Z4xLVgAL70EW7f24UtIkvbQ1pjHCVW1\nrdneBpzQbC8AtnSdtwVYOMbxx5vj+5R0Wh8ukihJ/Te77QKqqpL0tXNp1apVAPzyl/DVrw7zu787\n3M/LS9K0NzIywsjIyLjf31Z4bEvyqqra2nRJPdkcfxw4ueu8k+i0OB5vtruPP76vi4+Gx6tfDd/4\nRh+rlqQZYnh4mOHh4V/vf/rTn+7p/W11W90BLG+2lwO3dx2/NMmcJIuAIWBNVW0FdiQ5txlAv6zr\nPfvkjCtJmhgTvjxJkluAtwPH0Rnf+FPga8CtwCnAZuDiqnqmOf8q4EPALuDKqvpmc/wNwJeAucCd\nVfWxfXxejX6nF16AefNg+3aYO3eivqEkTX+9Lk8yI9e26v5OS5fC9dfDG9/YYlGSNMW5ttVe7LqS\npP4biPBwuq4k9ddAhIctD0nqrxk/5vHUUzA0BE8/3blxUJL0co557OX44+Goo+DRR9uuRJJmjhkf\nHmDXlST1m+EhSerZwISHM64kqX8GJjxseUhS/8z42VYAL74IxxwDTzzR+a8kaU/OthrDrFlwxhnw\n4IMHPleSdGADER5g15Uk9ZPhIUnqmeEhSerZQAyYAzz7LCxYADt2dMZAJEm7OWC+D/PmwW/8Bmza\n1HYlkjT9DUx4AJx9tl1XktQPAxUejntIUn8YHpKknhkekqSeDVR4nHpqZ9bVz37WdiWSNL0NVHgc\ndhicdZatD0k6VAMVHmDXlST1Q6vhkWRlknVJHkxyc5IjkhybZHWSh5PcnWT+XudvTLIhyfnj+Uyn\n60rSoWstPJKcCnwYeH1VnQnMAi4FVgCrq2oxcE+zT5IlwCXAEmAZcF2Snuu35SFJh67NlscOYCdw\nZJLZwJHAT4ELgJuac24CLmq2LwRuqaqdVbUZ2ASc0+uH/tZvwY9/DDt3HmL1kjTAWguPqtoO/Dnw\nEzqh8UxVrQZOqKptzWnbgBOa7QXAlq5LbAEW9vq5Rx4Jp5wCGzaMu3RJGniz2/rgJK8B/hg4FXgW\n+MckH+g+p6oqyf5WbhzztVWrVv16e3h4mOHh4T1eH32m+ZlnjqdySZr+RkZGGBkZGff7W1tVN8kl\nwO9U1X9o9i8DzgPeCbyjqrYmORG4t6pOS7ICoKquac6/C7i6qu7b67pjrqrb7TOfge3b4c/+rO9f\nS5Kmpem0qu4G4Lwkc5MEeDewHvg6sLw5Zzlwe7N9B3BpkjlJFgFDwJrxfLAzriTp0LTWbVVVDyT5\nMvB94CXgB8AXgaOBW5NcDmwGLm7OX5/kVjoBswu44oBNjH0YnXFVBTnonJUkjRqYh0F1q4Ljj4cH\nH4QTT5ykwiRpCptO3VatSbzfQ5IOxUCGBxgeknQoBjo81q5tuwpJmp4GOjxseUjS+AzkgDnACy/A\nvHmd+z3mzp2EwiRpCnPA/CDNmQOLF8O6dW1XIknTz8CGB9h1JUnjZXgYHpLUs4EPD2dcSVLvBnbA\nHOCpp2BoCJ5+2mVKJA02B8x7cPzxned7PPpo25VI0vQy0OEBrrArSeMx8OHhoLkk9c7wMDwkqWeG\nh+EhST0b6NlWALt2dZYpeeIJOOaYCSxMkqYwZ1v1aPZsWLKk82AoSdLBGfjwAGdcSVKvDA8c95Ck\nXhkeGB6S1KuBHzAHePZZWLiw899ZsyaoMEmawhwwH4d58zpLlWza1HYlkjQ9GB4Nu64k6eC1Gh5J\n5ie5LclDSdYnOTfJsUlWJ3k4yd1J5nedvzLJxiQbkpzfz1oMD0k6eG23PK4F7qyq04GzgA3ACmB1\nVS0G7mn2SbIEuARYAiwDrkvSt/qdritJB6+18EgyD3hrVd0AUFW7qupZ4ALgpua0m4CLmu0LgVuq\namdVbQY2Aef0qx5bHpJ08NpseSwCnkpyY5IfJPmbJEcBJ1TVtuacbcAJzfYCYEvX+7cAC/tVzKmn\ndmZbbd/erytK0sw1u+XPfj3w0ar6XpLP0XRRjaqqSrK/ebdjvrZq1apfbw8PDzM8PHzAYg47DM46\nq9P6eMc7Dly8JE1nIyMjjIyMjPv9rd3nkeRVwHeqalGz/9vASuDVwDuqamuSE4F7q+q0JCsAquqa\n5vy7gKur6r69rtvzfR6jPvIReO1r4eMfH/fXkqRpadrc51FVW4HHkixuDr0bWAd8HVjeHFsO3N5s\n3wFcmmROkkXAELCmnzU57iFJB6fNbiuAPwL+Pskc4BHgg8As4NYklwObgYsBqmp9kluB9cAu4Ipx\nNzH2YelS+MIX+nlFSZqZXJ6kyy9/Cccd1xk4P/zwPhcmSVPYtOm2moqOPBJOOQU2bGi7Ekma2gyP\nvTjuIUkHZnjsZelSWLu27SokaWozPPZiy0OSDszw2MtoeMyweQSS1FeGx14WLoSXXoKtW9uuRJKm\nLsNjL4ldV5J0IIbHGAwPSdo/w2MMhock7Z/hMQan60rS/rk8yRiefx7mz+8822Pu3D4VJklTWN+W\nJ0nyiyQ/38ef/5vku0ne3Z+yp5YjjoChIVi3ru1KJGlq2uequlX1yn29lmQ2cAZwc/PfGWf0meZv\nfGPblUjS1DOuMY/meeMPAJ/vcz1ThoPmkrRvhzRgXlV/3a9CphrDQ5L2zQHzfXjqqc64x9NPd24c\nlKSZzOd59Mnxx3ee7/Hoo21XIklTj+GxH3ZdSdLYDI/9MDwkaWyGx36MTteVJO3J8NgPWx6SNDZn\nW+3Hrl0wb17n2R5HH92XS0rSlORsqz6aPRuWLIEf/rDtSiRpamk9PJLMSnJ/kq83+8cmWZ3k4SR3\nJ5nfde7KJBuTbEhy/mTUZ9eVJL1c6+EBXAmsB0b7mlYAq6tqMXBPs0+SJcAlwBJgGXBdkgmv3/CQ\npJdrNTySnAS8B/hbYLSv7QLgpmb7JuCiZvtC4Jaq2llVm4FNwDkTXaMzriTp5dpuefwF8Angpa5j\nJ1TVtmZ7G3BCs70A2NJ13hZg4UQXeNZZ8KMfwYsvTvQnSdL0sc8l2SdakvcCT1bV/UmGxzqnqirJ\n/qZOjfnaqlWrfr09PDzM8PCYlz8o8+Z1lip55BFYvHjcl5GkKWVkZISRkZFxv7+1qbpJPgNcBuwC\nXgEcA3wFeBMwXFVbk5wI3FtVpyVZAVBV1zTvvwu4uqru2+u6fZuqO+qii+D974eLL+7rZSVpypg2\nU3Wr6qqqOrmqFgGXAv9cVZcBdwDLm9OWA7c323cAlyaZk2QRMASsmYxaHTSXpD21PebRbbS5cA3w\nO0keBt7Z7FNV64Fb6czM+gZwRd+bGPtgeEjSnrzD/CA88ggMD8Njj/X1spI0ZUybbqvpZNEiePZZ\n2L697UokaWowPA7CYYd1puzadSVJHYbHQXLcQ5J2MzwO0tKlsHZt21VI0tRgeBwkWx6StJuzrQ7S\nc8/BccfBjh1w+OF9v7wktcrZVhPkqKPglFNgw4a2K5Gk9hkePXCFXUnqMDx64LiHJHUYHj0wPCSp\nw/Doweh03Rk2x0CSemZ49GDhws5DobZubbsSSWqX4dGDxK4rSQLDo2fOuJIkw6NntjwkyfDomeEh\nSS5P0rPnn4f58zvP9pg7d8I+RpImlcuTTLAjjoChIVi3ru1KJKk9hsc42HUladAZHuNgeEgadIbH\nODhdV9Kgc8B8HJ56ChYv7gya56CHlyRp6nLAfBIcf3xnptVPftJ2JZLUjtbCI8nJSe5Nsi7Jj5J8\nrDl+bJLVSR5OcneS+V3vWZlkY5INSc5vq3bwmeaSBlubLY+dwMer6gzgPOAjSU4HVgCrq2oxcE+z\nT5IlwCXAEmAZcF2S1up30FzSIGvtL9+q2lpVa5vtXwAPAQuBC4CbmtNuAi5qti8EbqmqnVW1GdgE\nnDOpRXcxPCQNsikx5pHkVOB1wH3ACVW1rXlpG3BCs70A2NL1ti10wqYVzriSNMhmt11AklcC/wRc\nWVU/T9f0paqqJPubOjXma6tWrfr19vDwMMPDw32ptdvQEDzxBPz853D00X2/vCRNqJGREUZGRsb9\n/lan6iY5HPjvwDeq6nPNsQ3AcFVtTXIicG9VnZZkBUBVXdOcdxdwdVXdt9c1J3yq7qg3vQmuvRbe\n/OZJ+ThJmjDTZqpuOk2M64H1o8HRuANY3mwvB27vOn5pkjlJFgFDwJrJqncszriSNKja7LZ6C/AB\n4IdJ7m+OrQSuAW5NcjmwGbgYoKrWJ7kVWA/sAq6YtCbGPjhoLmlQeYf5IfjWt+BP/gS++91J+ThJ\nmjC9dlsZHofgmWfgpJPg2Wdh1qxJ+UhJmhDTZsxjJpg/v7NUySOPtF2JJE0uw+MQOe4haRAZHofI\n8JA0iAyPQ+R0XUmDyPA4RLY8JA0iw+MQLVrUmW21fXvblUjS5DE8DtFhh8GZZ9r6kDRYDI8+cIVd\nSYPG8OgDxz0kDRrDow+ccSVp0Lg8SR889xwcdxzs2AGHHz6pHy1JfeHyJC046ig45RTYsKHtSiRp\nchgefeK4h6RBYnj0iTOuJA0Sw6NPbHlIGiSGR5+MzriaYfMPJGlMhkefLFwIL74IW7e2XYkkTTzD\no08Su64kDQ7Do48MD0mDwvDoI8ND0qAwPPrI6bqSBoXLk/TR88/D/PmdZ3vMndtKCZI0LjN+eZIk\ny5JsSLIxySfbrqfbEUfA0BCsW9d2JZI0saZVeCSZBfwVsAxYArwvyentVrUnxz0kDYJpFR7AOcCm\nqtpcVTuBfwAubLmmPRgekgbB7LYL6NFC4LGu/S3AuS3VMqalS+Hzn4ddu9quRJImznQLj4MaCV+1\natWvt4eHhxkeHp6gcl7ubW+DT33K8JA0tT388AgbN46M+/3TarZVkvOAVVW1rNlfCbxUVf+l65zW\nZltJ0nQ102dbfR8YSnJqkjnAJcAdLdckSQNnWnVbVdWuJB8FvgnMAq6vqodaLkuSBs606rY6GHZb\nSVLvZnq3lSRpCjA8JEk9MzwkST0zPCRJPTM8JEk9MzwkST0zPCRJPTM8JEk9MzwkST0zPCRJPTM8\nJEk9MzwkST0zPCRJPTM8JEk9MzwkST0zPCRJPTM8JEk9MzwkST0zPCRJPTM8JEk9MzwkST0zPCRJ\nPWslPJJ8NslDSR5I8pUk87peW5lkY5INSc7vOv6GJA82r13bRt2SpI62Wh53A2dU1VLgYWAlQJIl\nwCXAEmAZcF2SNO/5AnB5VQ0BQ0mWTX7Z08vIyEjbJUwZ/ha7+Vvs5m8xfq2ER1WtrqqXmt37gJOa\n7QuBW6pqZ1VtBjYB5yY5ETi6qtY0530ZuGgya56O/D/Gbv4Wu/lb7OZvMX5TYczjQ8CdzfYCYEvX\na1uAhWMcf7w5LklqweyJunCS1cCrxnjpqqr6enPOp4AXqurmiapDktR/qap2Pjj5A+DDwLuq6lfN\nsRUAVXVNs38XcDXwKHBvVZ3eHH8f8Paq+sMxrtvOF5Kkaa6qcuCzOias5bE/zWD3J+gEwK+6XroD\nuDnJf6XTLTUErKmqSrIjybnAGuAy4C/HunYvX16SND6ttDySbATmANubQ9+pqiua166iMw6yC7iy\nqr7ZHH8D8CVgLnBnVX1ssuuWJHW01m0lSZq+psJsq75Isqy5sXBjkk+2XU9bkpyc5N4k65L8KMnA\nt9CSzEpyf5Kvt11Lm5LMT3Jbc4Pu+iTntV1TW5qbkdc1Nx7fnOSItmuaLEluSLItyYNdx45NsjrJ\nw0nuTjL/QNeZEeGRZBbwV3RuLFwCvC/J6e1W1ZqdwMer6gzgPOAjA/xbjLoSWA8MejP7WjpdvqcD\nZwEPtVxPK5KcSmeyzuur6kxgFnBpmzVNshvp/F3ZbQWwuqoWA/c0+/s1I8IDOAfYVFWbq2on8A90\nbjgcOFW1tarWNtu/oPMXxIJ2q2pPkpOA9wB/CwzsZIpmCaC3VtUNAFW1q6qebbmstuyg84+sI5PM\nBo6kc+/YQKiqbwNP73X4AuCmZvsmDuIm7JkSHguBx7r2R28uHGjNv7BeR+cu/kH1F3Rm9r10oBNn\nuEXAU0luTPKDJH+T5Mi2i2pDVW0H/hz4CfBT4Jmq+h/tVtW6E6pqW7O9DTjhQG+YKeEx6N0RL5Pk\nlcBtdGas/aLtetqQ5L3Ak1V1PwPc6mjMBl4PXFdVrwee4yC6JmaiJK8B/hg4lU6r/JVJ/n2rRU0h\n1ZlFdcC/U2dKeDwOnNy1fzJ7LmcyUJIcDvwT8HdVdXvb9bTozcAFSf4PcAvwziRfbrmmtmwBtlTV\n95r92+iEySB6I/C/q+pnVbUL+Aqd/60Msm1JXgXQrCX45IHeMFPC4/t0Vto9NckcOivz3tFyTa1o\nViG+HlhfVZ9ru542VdVVVXVyVS2iMyD6z1X1+23X1Yaq2go8lmRxc+jdwLoWS2rTBuC8JHOb/7+8\nm86EikF2B7C82V4OHPAfna3cYd5vVbUryUeBb9KZOXF9VQ3kTBLgLcAHgB8mub85trKq7mqxpqli\n0Ls3/wj4++YfWI8AH2y5nlZU1QNNC/T7dMbCfgB8sd2qJk+SW4C3A8cleQz4U+Aa4NYklwObgYsP\neB1vEpQk9WqmdFtJkiaR4SFJ6pnhIUnqmeEhSeqZ4SFJ6pnhIUnqmeEhdUnyi+a/v9k87rif175q\nr/1/6ef1pclkeEh7Gr3xaRHw/l7e2KzQuj8r9/igqrf0cn1pKjE8pLFdA7y1eYjUlUkOS/LZJGuS\nPJDkPwIkGU7y7SRfA37UHLs9yfebh3F9uDl2DTC3ud5/a46NtnLSXPvBJD9McnHXtUeS/GPzAKe/\nGy0uyTXNw4weSPLZSf1lJGbI8iTSBPgk8J+r6t8CNGHxTFWd0zx17n8lubs593XAGVX1aLP/wap6\nOslcYE2S26pqRZKPVNXruj5jtJXz74CldB7QdDzwvSTfal47m84Dzp4A/iXJW+iszXRRVZ3W1HbM\nBHx/ab9seUhj23sJ9/OB32/WC/sucCzw2ua1NV3BAXBlkrXAd+is8Dx0gM/6beDm6ngS+J/Am+iE\ny5qq+mmzTPZa4DeBZ4BfJbk+ye8B/2/c31IaJ8NDOngfrarXNX9e0/UAoedGT0gyDLwLOK+qzgbu\nB15xgOsWLw+r0VbJ813HXgQOr6oX6Tw98zbgvYCLXmrSGR7S2H4OHN21/03gitFB8SSL9/EkvmOA\np6vqV0lOo/Mc+VE79zGo/m3gkmZc5XjgbcAa9vEAqyRHAfOr6hvAf6LT5SVNKsc8pD2N/ov/AeDF\npvvpRuAv6Tx57gfNMyCeBH6vOb97aeq7gD9Msh74MZ2uq1FfpLNU/r9W1WWj76uqryb5N81nFvCJ\nqnoyyem8fBn5ohNqX0vyCjoB8/G+fHOpBy7JLknqmd1WkqSeGR6SpJ4ZHpKknhkekqSeGR6SpJ4Z\nHpKknhkekqSeGR6SpJ79f6X79EmGjwGYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bff4c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta, J = gradDescent(initial_theta,x,y,0.1,1)\n",
    "plt.plot(J)\n",
    "plt.ylabel('J')\n",
    "plt.xlabel('Iterations');\n",
    "print('theta: ',theta.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent again (Correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NB have to define m as float otherwise i get 0\n",
    "def gradientDescent(x, y, theta, alpha=0.01, num_iters=1500):\n",
    "    m = y.size\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    for iter in np.arange(num_iters):\n",
    "        h = x.dot(theta)\n",
    "        theta = theta - alpha*(1/float(m))*(x.T.dot(h-y))\n",
    "        J_history[iter] = ComputeCost(theta,x,y)\n",
    "    return(theta, J_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('theta: ', array([ 22.53279993,  -1.03951946,   0.72056119,  -0.52921817,\n",
      "        -1.87070616,   4.6072502 ,  -1.34490849,  -3.16050136,\n",
      "        -0.03728437,  -2.03761574,   1.2349559 ]))\n",
      "[ 22.53280632  -1.06817758   0.76463106  -0.55720013  -2.02234253\n",
      "   4.57127478  -1.35639418  -3.33394924   0.03534378  -2.06091784\n",
      "   1.22754427]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEVCAYAAAAGrllxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgEzsxJzCJiZlZhDwMysxBwCZmYl9v8BBV7oOPGZ8wUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x190ef1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# theta for minimized cost J\n",
    "theta , Cost_J = gradientDescent(x, y, initial_theta)\n",
    "print('theta: ',theta.ravel())\n",
    "\n",
    "plt.plot(Cost_J)\n",
    "plt.ylabel('Cost J')\n",
    "plt.xlabel('Iterations');\n",
    "print Beta1 # very similar answers for analytical and gradient descent methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GradReg(theta,x,y,lamb):\n",
    "    # remember index 1 to end because we dont regularise the constant\n",
    "    m = y.size\n",
    "    h = x.dot(theta)\n",
    "    grad =  (1/float(m)) * (x.T.dot(h-y)) \n",
    "    grad[1:] = grad[1:] + lamb/float(m) * theta[1:]\n",
    "    return grad       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost with Regularisation is: 75805464.02\n",
      "Gradient with Regularisation is: [[-22.53280632]\n",
      " [  3.5415213 ]\n",
      " [ -3.30850184]\n",
      " [  4.44007841]\n",
      " [  3.9223466 ]\n",
      " [ -6.38265888]\n",
      " [  3.46003881]\n",
      " [ -2.29407785]\n",
      " [  4.3006576 ]\n",
      " [  4.66093742]\n",
      " [ -3.06081285]]\n"
     ]
    }
   ],
   "source": [
    "cost = regCostFunction(initial_theta,x,y,1)\n",
    "gradient = GradReg(initial_theta,x,y,1)\n",
    "print('Cost with Regularisation is: {}'.format(cost))\n",
    "print('Gradient with Regularisation is: {}'.format(gradient))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-22.53280632],\n",
       "       [  3.5415213 ],\n",
       "       [ -3.30850184],\n",
       "       [  4.44007841],\n",
       "       [  3.9223466 ],\n",
       "       [ -6.38265888],\n",
       "       [  3.46003881],\n",
       "       [ -2.29407785],\n",
       "       [  4.3006576 ],\n",
       "       [  4.66093742],\n",
       "       [ -3.06081285]])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an optimisation function to minimse Cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainLinReg(theta,x,y,lamb):\n",
    "    # optimisation function: minimize from scipy\n",
    "    opt = minimize(regCostFunction, theta, args=(x,y,lamb), method='BFGS', jac=GradReg,\n",
    "                   options={'maxiter':5000})\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainLinReg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ea3c97572610>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainLinReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'trainLinReg' is not defined"
     ]
    }
   ],
   "source": [
    "fit = trainLinReg(initial_theta,x.T,y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
